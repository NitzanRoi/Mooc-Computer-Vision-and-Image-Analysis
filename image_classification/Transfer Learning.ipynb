{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Transfer Learning\n",
    "\n",
    "In this lab, you must use transfer learning to train a convolutional neural network that classifies vehicle images.\n",
    "\n",
    "> **Hints**:\n",
    "> - See the [PyTorch](https://pytorch.org/docs/stable/torchvision/models.html) and [Keras](https://keras.io/applications/) documentation for details of available base models - bear in mind that some models may incur too much processing overhead to be trained in the Azure Notebooks environment.\n",
    "> - Use the images in the **../data/classification/train** folder to train the model.\n",
    "> - Use the images in the **../data/classification/test** folder to test the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount files:\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "import os, os.path\n",
    "folder = 'Moocs/edx_Microsoft'\n",
    "os.chdir('/content/drive/My Drive/'+folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup:\n",
    "!pip install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n",
    "!pip install torchvision\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data: now with Data-Augmentation\n",
    "import os\n",
    "\n",
    "def make_class_list(train_folder):\n",
    "    return sorted(os.listdir(train_folder))\n",
    "\n",
    "def load_data(train_folder):\n",
    "    \n",
    "    # Load all of the images\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        # # Randomly augment the image data\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=train_folder,\n",
    "        transform=transformation\n",
    "    )\n",
    "    \n",
    "    return transformation, full_dataset\n",
    "    \n",
    "def split_data(full_dataset):\n",
    "    # Split into training 70% and testing 30% datasets\n",
    "    train_ratio = 0.7\n",
    "        \n",
    "    train_size = int(train_ratio * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = \\\n",
    "        torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def define_loader(train_dataset, test_dataset):    \n",
    "    size_of_batch = 15\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=size_of_batch,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=size_of_batch,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def main1():\n",
    "    train_folder = \"./data/classification/training\"\n",
    "    class_list = make_class_list(train_folder)\n",
    "    transformation, full_dataset = load_data(train_folder)\n",
    "    train_dataset, test_dataset = split_data(full_dataset)\n",
    "    train_loader, test_loader = define_loader(train_dataset, test_dataset)\n",
    "    \n",
    "#     print(class_list)\n",
    "#     print(len(train_loader.dataset))\n",
    "    \n",
    "    return train_loader, test_loader, class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model:\n",
    "\n",
    "def get_pretrained_model(class_list):\n",
    "    model_resnet = torchvision.models.resnet18(pretrained=True)\n",
    "    for param in model_resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    num_ftrs = model_resnet.fc.in_features\n",
    "\n",
    "    # adding linear layer\n",
    "    model_resnet.fc = nn.Linear(num_ftrs, len(class_list))\n",
    "    return model_resnet\n",
    "        \n",
    "def main2():\n",
    "    train_loader, test_loader, class_list = main1()\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if (torch.cuda.is_available()):\n",
    "        # if GPU available, use cuda\n",
    "        device = \"cuda\"\n",
    "    \n",
    "    # get the pretrained model\n",
    "    model = get_pretrained_model(class_list).to(device)\n",
    "#     print(model)\n",
    "    return model, device, train_loader, test_loader, class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test:\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, loss_criteria):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    \n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "               \n",
    "def test(model, device, test_loader, loss_criteria):\n",
    "    # Switch the model to evaluation mode (without backpropagate)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                                        avg_loss, correct, len(test_loader.dataset),\n",
    "                                        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss\n",
    "\n",
    "def run_all(model, device, train_loader, test_loader):\n",
    "    # Track metrics in these arrays\n",
    "    epoch_nums = []\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "#     Use an \"Adam\" optimizer to adjust weights\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Specify the loss criteria\n",
    "    loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train epochs\n",
    "    epochs = 3\n",
    "    print('Training on', device)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch, loss_criteria)\n",
    "        test_loss = test(model, device, test_loader, loss_criteria)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)\n",
    "    \n",
    "    return epoch_nums, training_loss, validation_loss  \n",
    "    \n",
    "def main3():\n",
    "    model, device, train_loader, test_loader, class_list = main2()\n",
    "    epoch_nums, training_loss, validation_loss = run_all(model, device, train_loader, test_loader)\n",
    "    print('finish training')\n",
    "    return epoch_nums, training_loss, validation_loss, model, test_loader, class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view loss graph:\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# globals for other cells\n",
    "epoch_nums, training_loss, validation_loss, model, test_loader, class_list = main3()\n",
    "\n",
    "def plot_loss():\n",
    "    plt.plot(epoch_nums, training_loss)\n",
    "    plt.plot(epoch_nums, validation_loss)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['training', 'validation'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix():\n",
    "    truelabels = []\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    print(\"Getting predictions from test set...\")\n",
    "    for data, target in test_loader:\n",
    "        for label in target.data.numpy():\n",
    "            truelabels.append(label)\n",
    "            \n",
    "        if (torch.cuda.is_available()):\n",
    "            # if GPU available, use cuda\n",
    "            for prediction in model(data.cuda()).cpu().data.numpy().argmax(1):\n",
    "                predictions.append(prediction)\n",
    "        else:\n",
    "            for prediction in model(data).data.numpy().argmax(1):\n",
    "                predictions.append(prediction)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    cm = confusion_matrix(truelabels, predictions)\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_list))\n",
    "    plt.xticks(tick_marks, class_list, rotation=85)\n",
    "    plt.yticks(tick_marks, class_list)\n",
    "    plt.xlabel(\"Predicted Shape\")\n",
    "    plt.ylabel(\"True Shape\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and deploy:\n",
    "\n",
    "model_file = 'my-pretrained-model.pt'\n",
    "torch.save(model.state_dict(), model_file)\n",
    "print(\"Model saved\")\n",
    "\n",
    "# Delete the existing model variable\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for classification:\n",
    "\n",
    "# Helper function to resize image\n",
    "def resize_image(src_img, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image\n",
    "\n",
    "    # rescale the image so the longest edge is the right size\n",
    "    src_img.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new image of the right shape\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the rescaled image onto the new background\n",
    "    new_image.paste(src_img, (int((size[0] - src_img.size[0]) / 2), int((size[1] - src_img.size[1]) / 2)))\n",
    "    \n",
    "    # return the resized image\n",
    "    return new_image\n",
    "\n",
    "# Function to predict the class of an image\n",
    "def predict_image(classifier, image_array):\n",
    "    \n",
    "    # Set the classifer model to evaluation mode\n",
    "    classifier.eval()\n",
    "    \n",
    "    # These are the classes our model can predict\n",
    "    class_names = class_list\n",
    "    \n",
    "    # Apply the same transformations as we did for the training images\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Preprocess the imagees\n",
    "    image_tensor = torch.stack([transformation(image).float() for image in image_array])\n",
    "\n",
    "    # Turn the input into a Variable\n",
    "    input_features = image_tensor\n",
    "\n",
    "    # Predict the class of each input image\n",
    "    predictions = classifier(input_features)\n",
    "    \n",
    "    predicted_classes = []\n",
    "    # Convert the predictions to a numpy array \n",
    "    for prediction in predictions.data.numpy():\n",
    "        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n",
    "        # So get the index of the highest probability\n",
    "        class_idx = np.argmax(prediction)\n",
    "        # And append the corresponding class name to the results\n",
    "        predicted_classes.append(class_names[class_idx])\n",
    "    return np.array(predicted_classes)\n",
    "\n",
    "print(\"Functions created - ready to use model for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification:\n",
    "\n",
    "from random import randint\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_model():\n",
    "    model = get_pretrained_model(class_list)\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    return model\n",
    "\n",
    "def plot_classes(predictions, image_arrays):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    # plot each image with its corresponding prediction\n",
    "    for idx in range(len(predictions)):\n",
    "        a=fig.add_subplot(1,len(predictions),idx+1)\n",
    "        imgplot = plt.imshow(image_arrays[idx])\n",
    "        a.set_title(predictions[idx])\n",
    "\n",
    "\n",
    "def classify(test_image_files, test_folder, size, background_color, model):\n",
    "    # Empty array on which to store the images\n",
    "    image_arrays = []\n",
    "    # Get the images and show the predicted classes\n",
    "    for file_idx in range(len(test_image_files)):\n",
    "        img = Image.open(os.path.join(test_folder, test_image_files[file_idx]))\n",
    "        \n",
    "        # resize the image so it matches the training set -\n",
    "        # it  must be the same size as the images on which the model was trained\n",
    "        resized_img = np.array(resize_image(img, size, background_color))\n",
    "                        \n",
    "        # Add the image to the array of images\n",
    "        image_arrays.append(resized_img)\n",
    "\n",
    "    # Get predictions from the array of image arrays\n",
    "    # Note that the model expects an array of 1 or more images - just like the batches on which it was trained\n",
    "    return predict_image(model, np.array(image_arrays)), image_arrays\n",
    "\n",
    "def main4():\n",
    "    model = get_model()\n",
    "\n",
    "    test_folder = './data/classification/test'\n",
    "    test_image_files = os.listdir(test_folder)\n",
    "    size = (224,224)\n",
    "    background_color=\"white\"\n",
    "\n",
    "    predictions, image_arrays = classify(test_image_files, test_folder, size, background_color, model)\n",
    "    plot_classes(predictions, image_arrays)\n",
    "\n",
    "main4()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "name": "Lab 02 -  Transfer Learning.ipynb",
   "provenance": [],
   "private_outputs": true,
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}